---
title: "BST 222 Final Reoprt"
author: "Zach Clement, Nona"
date: "11/30/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Introduction
The purpose of this study is to examine the impacts of a phenomenon in regression analysis and prediction known as “sparsity”. In plain English, the term sparsity refers to the state of being thinly scattered or distributed. This definition has clear connections to sparsity in a regression model, which we will consider as the percent of true coefficients that are exactly equal to zero. Having high sparsity implies that the model under consideration has many predictors that are not impactful and could therefore be discarded to achieve a more parsimonious, or simple relationship with the outcome. In real world applications, it is often advantageous to obtain a model that has fewer predictors as this allows for ease of interpretation and stronger effects from each individual covariate. For example, in the case of genomics research, a setting where the number of predictors far exceeds the sample size, the number of meaningful (non-zero) features in the data cannot be discerned using the traditional Ordinary Least Squares approach. Even the elegant regularization procedure of Ridge regression is unable to remove parameters since it only reduces the magnitude of coefficients, and thus does not force them to zero.

The above example makes it clear that a more powerful technique is warranted to better estimate our parameters of interest while still maintaining a degree of parsimony in model structure. In 1996, Stanford Statistics Professor Robert Tibishirani published a paper titled “Regression Shrinkage and Selection via the LASSO, which presented the Least Absolute Shrinkage and Selection Operator (LASS) as an alternative to the existing regularization options. The LASSO introduces a new form of regularization that effectively “shrinks” estimates for nil coefficients to be exactly zero. Fifteen year later, Tibishirani’s Stanford colleagues, Professors Trevor Hastie and Hui Zou, developed the Elastic-Net selection procedure as a way to strike a balance between the flexibility of Ridge regression and the severity of LASSO. There now exist a myriad of estimation approaches, with each having advantages and disadvantages that we will explore further in this paper.

With the motivation given, we will now turn attention to our underlying research questions:

1.) How does sparsity impact the Mean Squared Error of established regression methods like OLS and Ridge, and how do these perform relative to the LASSO and Elastic-Net procedures?

2.) What situations allow for the penalized regression methods to achieve lower variance relative to OLS at the cost of higher bias?

3.) How do the four methods above perform for larger sample sizes and number of simulations?

### Methods

We will now present the main statistical methods that were utilized in our investigation. In order to examine the effects of sparsity in linear models, we looked at four distinct model methods that deal with sparsity in different ways. These four methods are: 

- Ordinary Least Squares
- The Least Absolute Shrinkage and Selection Operator (LASSO)
- Ridge Regression
- Elastic-Net

#### Ordinary Least Squares
Ordinary Least Squares, or OLS, is a method in linear regression that chooses parameters of a linear model based on a set of explanatory variables based on the principle of least squares. The Least Squares Estimation process is based on the process of minimizing the distance between the actual data responses and the fitted values/residuals of the estimated linear regression line. The line produced by the method of least squares is shown below:

In other words, the ordinary least squares method chooses estimates so that the regression model deviates the least from the data, with no penalty term. From the least square estimates we can then find variance

#### LASSO

#### Ridge Regression

#### Elastic Net


### Metrics

### Design of Simulation Study  


### Results
```{r}
library(plyr)
library(dplyr)
n_reps <- 500
beta_list <- c(2,  1, .5, -.5, 0, 0, 0, 0, 0, 0, 0)
num_obs <- 300


##These lines may cause errors on your machine, see below if they don't work
future::plan(multisession, workers = parallel::detectCores()) 
df <- future.apply::future_replicate(n_reps, model_fit(beta_list, num_obs)) %>% t() %>% data.frame()
future::plan(sequential) #I think this should reset your R session because we don't need to parallel process anything else

#The commented line below will do the same thing as the above two lines, but slower. 
# df <- replicate(n_reps, model_fit(beta_list, num_obs)) %>% t() %>% data.frame()


plot_all_beta(df, beta_list)

```

### Summary/Conclusion

