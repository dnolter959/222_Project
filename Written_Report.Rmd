---
title: "BST 222 Final Reoprt"
author: "Zach Clement, Nona Jiang, Dan Nolte, Willow Duffell, Addison McGhee"
date: "11/30/2021"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(pander)
library(tidyverse)
library(ggplot2)
library(ggpubr)
library(future.apply)
library(knitr)
library(dplyr)
library(glmnet)
```

### I) Introduction
The purpose of this study is to examine the impacts of a phenomenon in regression analysis and prediction known as “sparsity”. In plain English, the term sparsity refers to the state of being thinly scattered or distributed. This definition has clear connections to sparsity in a regression model, which we will consider as the percent of true coefficients that are exactly equal to zero. Having high sparsity implies that the model under consideration has many predictors that are not impactful and could therefore be discarded to achieve a more parsimonious, or simple relationship with the outcome. In real world applications, it is often advantageous to obtain a model that has fewer predictors as this allows for ease of interpretation and stronger effects from each individual covariate. For example, in the case of genomics research, a setting where the number of predictors far exceeds the sample size, the number of meaningful (non-zero) features in the data cannot be discerned using the traditional Ordinary Least Squares approach. Even the elegant regularization procedure of Ridge regression is unable to remove parameters since it only reduces the magnitude of coefficients, and thus does not force them to zero.

The above example makes it clear that a more powerful technique is warranted to better estimate our parameters of interest while still maintaining a degree of parsimony in model structure. In 1996, Stanford Statistics Professor Robert Tibishirani published a paper titled “Regression Shrinkage and Selection via the LASSO, which presented the Least Absolute Shrinkage and Selection Operator (LASSO) as an alternative to the existing regularization options. The LASSO introduces a new form of regularization that effectively “shrinks” estimates for nil coefficients to be exactly zero. Fifteen years later, Tibishirani’s Stanford colleagues, Professors Trevor Hastie and Hui Zou, developed the Elastic-Net selection procedure as a way to strike a balance between the flexibility of Ridge regression and the severity of LASSO. There now exist a myriad of estimation approaches, with each having advantages and disadvantages that we will explore further in this paper.

With the motivation given, we will now turn attention to our underlying research questions:

1.) How does sparsity impact the Mean Squared Error of established regression methods like OLS and Ridge, and how do these perform relative to the LASSO and Elastic-Net procedures?

2.) What situations allow for the penalized regression methods to achieve lower variance relative to OLS at the cost of higher bias?

3.) How do the four methods above perform for larger sample sizes and number of simulations?

### 2) Methods

#### 2a) Models
We will now present the main statistical methods that were utilized in our investigation. In order to examine the effects of sparsity in linear models, we looked at four distinct model methods that deal with sparsity in different ways. These four methods are: 

- Ordinary Least Squares
- The Least Absolute Shrinkage and Selection Operator (LASSO)
- Ridge Regression
- Elastic-Net

##### Ordinary Least Squares
Ordinary Least Squares, or OLS, is a method in linear regression that chooses parameters of a linear model based on a set of explanatory variables based on the principle of least squares. The Least Squares Estimation process is based on the process of minimizing the distance between the actual data responses and the fitted values/residuals of the estimated linear regression line. The line produced by the method of least squares is shown below:
Ordinary Least Squares, or OLS, is a method in linear regression that estimates parameters of a linear model based on a set of explanatory variables based on the principle of least squares. A model with p explanatory variables would be modeled as such:

$Y = \beta_{0} + \sum_{j=1}^{p} \beta_{j}X_{j} + \epsilon$

Where Y is the dependent variable, $\beta_0$ is the intercept, $X_{j}$ is the jth explanatory variable and $\epsilon$ is the random error term. 

Least squares, also known as the minimum square error (SSE), goes about an estimation process that is based on minimizing the sum of square distances between the true data responses and the predicted values. That is, we minimize the *residuals*, which can be defined as $Y_{i} - \hat{Y}_i = e_{i}$. In order to accomplish this goal, the method of least squares produces a line such that 

$Y_{i} - \hat{Y}_{i} = Y_{i} - (\hat{\beta}_{0} + \hat{B}_{1}X_{i}) = e_{i}$

is minimized over all observations by summing up all the squares of the residuals such that the sum $e^{2}_{1} + e^{2}_{2} + \dots + e^{2}_{n}$ is minimized. 


In other words, the ordinary least squares method chooses estimates so that the regression model deviates the least from the data, with no penalty term. 

**Things to note:**
The outputs of regression for the OLS estimator are unbiased estimators of the different $\beta$ terms. According to the Guass-Markov Theorem, each of the $\hat{\beta}$ variables are unbiased and have minimum variance among all unbiased linear estimators for $\beta$. However this estimator will work best with data with no sparsity and large $\beta$ terms. When we look towards models with more sparsity, OLS may not be the best method to use. 

In other words, the ordinary least squares method chooses estimates so that the regression model deviates the least from the data, with no penalty term. From the least square estimates we can then find variance

##### LASSO
Works better with more zero's

##### Ridge Regression
Works better with a lot of small beta's (0.01 - 0.05)

##### Elastic Net
Works better with medium sparsity

#### 2b) Metrics
Out of sample MSE. 

#### 2c) Design of Simulation Study

As discussed above, through this study we were interested in demonstrating the superiority of LASSO regression in those contexts with a high level of sparsity. We then consider other contexts where Ridge Regression, Elastic Net, and OLS are expected to outperform LASSO, and test our hypotheses using simulations. 

##### Step 1: Simulation set up and data generation
We would like to predict Y given a set of 10 covariates using the linear model below. 

${Y} = {\beta}_0 + {\beta}_1 x_1 + {\beta}_2 x_2 + ... + ... +{\beta}_{10} x_{10}$

We then set the true parameters as follows:

${Y} = 2 + 1 x_1 + 0.5 x_2 -0.5 x_3 + 0 x_4 + 0 x_5 + 0 x_6 + 0 x_7 + 0 x_8 + 0 x_9 + 0 x_{10}$

Note that 7 of the parameters are set to 0; we made this choice because we are primarily interested in comparing model performance in the presence of sparsity. 

Next, we are interested in generating a random dataset, and then running our 4 models of interest (OLS, LASSO, Ridge, Elastic Net) on this dataset to estimate each of the 11 $\beta_i$ values using each model. We then repeat this simulation and fit regression coefficients for each model 500 times, and compare the performance of the four models. Below we explain this process in greater detail:  

**Step 1a: Generating random data**

In order to generate our random dataset we simply select a sample size of 300, and sample from a standard normal distribution for every coviariate value for each of the 300 observations. Again, every single value of every covariate is sampled from the exact same standard normal distribution. We do this to introduce randomness into the data generating process while still adhering to the normality and equal variance assumptions required by the estimation procedures we wish to test. The head of this data frame looks like this:

```{r, echo=FALSE}
beta_list <- c(2,  1, .5, -.5, 0, 0, 0, 0, 0, 0, 0)
num_obs = 6 
xi_sd <- replicate(n = length(beta_list) - 1, rnorm(num_obs))
data.frame(xi_sd)
```

Using these randomly generated covariate values, we generate corresponding Y values given our choice of parameters by simply multiplying the design matrix corresponding to our random data by the vector of parameter values, and the adding an error term (as drawn from the standard normal distribution). Doing so we retrieve a single dataset of randomly drawn covariate values and their corresponding Y values given our choice of parameters. The head of this data frame looks like this:

```{r, echo=FALSE}
x = cbind(rep(1, num_obs), xi_sd)
y = beta_list %*% as.matrix(t(x)) + rnorm(num_obs) # create matrix with TRUE Y
data <- cbind(y = t(y), xi_sd)
data = data.frame(data)
names(data) <- c("Y", "X1", "X2", "X3", "X4", "X5", "X6", "X7", "X8", "X9", "X10")
data
```

**Step 2: Simulations Details**
Once we have our dataset of predictors $X_i$ and outcome $Y$, what remains is to run our four regression models of interest on this dataset to retrieve point estimates for the 11 $\beta$ coefficients. After running each of the models, we are interested in examining (1) the proximity of the estimated coefficients to their true underlying values in each case, as well as (2) the proximity of the predicted $\hat{Y}$ values to their true $Y$, measured in Mean Squared Error. 

Further, we are interested in repeating this experiment many times (we chose 500) and examining the variance of the distributions of the coefficient point estimates.

As for the details of our regression models, I'll consider them in two categories: (1) the un-penalized model (OLS), and (2) the penalization models (Ridge, LASSO, Elastic Net)   

1) *Un-penalized Model (OLS)*:
For this model, we simply regress Y on $X_1, X_2, ..., X_{10}$  using OLS using the below command. 

```{r, results="hide"}
lm(Y ~ X1:X10, data = data)
```

2) *Penalization Models*:
We used the same basic procedure for fitting each of these 3 models. For each we use the `glmnet` library in R and run a model in this form:

```{r, echo = FALSE}
alpha = 0.5
lambda = 0.6
```

```{r, results="hide"}
glmnet(y = data$Y, x = data[,2:ncol(data)], family = "gaussian", alpha = alpha, lambda = lambda)
```
Where we set alpha equal to 0, 0.5, and 1 for Ridge, Elastic Net, and LASSO, respectively. For selecting the lambda level, we first ran each model with no lambda parameter and used cross validation to select the lambda level associated with the lowest MSE in each case. Then we fit each penalization model with this lambda value.  

### 3) Results
```{r, echo = FALSE}
generate_data <- function(beta_list, # a numeric vector of beta coefficients
                          num_obs, ## number of observations in the output matrix
                          num_underlying_factors = 0, #number of underlying factors (0 is no multicollinearity)
                          covariates_per_factor = 0, #covariates per underlying factor
                          strength_collinearity = 0 #between 0 and 1, 1 is perfect collinearity
                          
                          ){
  ## Takes a list of beta coefficients and returns a matrix satisfying those constraints

  
  xi_sd <- replicate(n =length(beta_list) - 1, rnorm(num_obs)) %>% as.matrix()
  if(num_underlying_factors > 0 && covariates_per_factor > 1 && strength_collinearity > 0){
    k <- 2
    for (i in 1:num_underlying_factors){
      current_factor <- rnorm(num_obs)
      new_k <- k + covariates_per_factor
      if (new_k > ncol(xi_sd)){
        k <- 3 #we've already gone around once so we might as well mix up some of the collinearity groups
        new_k <- k + covariates_per_factor
      }
      
      
      current_to_add <- k:new_k
      for (j in current_to_add){
        xi_sd[,j] <- (xi_sd[,j]*(1 - strength_collinearity) + current_factor*strength_collinearity ) |> scale() #weight them and then normalize back to have a sd of 1. We will be dividing 
      }
    }
  }
  x = cbind(rep(1, num_obs), xi_sd)
  y = beta_list %*% as.matrix(t(x)) + rnorm(num_obs) # create matrix with TRUE Y
  
  mat_to_fit <- cbind(y = t(y), xi_sd) 
  return(mat_to_fit)
}





model_fit <- function(beta_list, num_obs) {
  mat_to_fit <- generate_data(beta_list, num_obs)
  
  current_lm <- lm(mat_to_fit[,1] ~ mat_to_fit[,2:ncol(mat_to_fit)])
  current_lm <- lm(mat_to_fit[,1] ~ mat_to_fit[,2:ncol(mat_to_fit)])
  
  lambda_lasso <- glmnet::cv.glmnet(y = mat_to_fit[,1], 
                                    nfolds = 5, #so it will be ok when we use on small samples
                                x = mat_to_fit[,2:ncol(mat_to_fit)],
                                family = "gaussian",
                                alpha = 1 #lasso penalty (0 is ridge)
  ) %>% 
    .$lambda.min
  
  lambda_elastic <- glmnet::cv.glmnet(y = mat_to_fit[,1], 
                                    x = mat_to_fit[,2:ncol(mat_to_fit)],
                                    nfolds = 5,
                                    family = "gaussian",
                                    alpha = .5 #lasso penalty (0 is ridge)
  ) %>% 
    .$lambda.min
  
  
  lambda_ridge <- glmnet::cv.glmnet(y = mat_to_fit[,1], 
                                    x = mat_to_fit[,2:ncol(mat_to_fit)],
                                    nfolds = 5,
                                    family = "gaussian",
                                    alpha = 0 #ridge penalty
  ) %>% 
    .$lambda.min
  
  current_lasso <- glmnet::glmnet(y = mat_to_fit[,1], 
                                  x = mat_to_fit[,2:ncol(mat_to_fit)],
                                  family = "gaussian",
                                  alpha = 1, #1 is lasso penalty (0 is ridge)
                                  lambda = lambda_lasso
                                  )
  
  current_ridge <- glmnet::glmnet(y = mat_to_fit[,1], 
                                  x = mat_to_fit[,2:ncol(mat_to_fit)],
                                  family = "gaussian",
                                  alpha = 0, #0 is ridge
                                  lambda = lambda_ridge
                              
  )
  
  current_elastic <- glmnet::glmnet(y = mat_to_fit[,1], 
                                  x = mat_to_fit[,2:ncol(mat_to_fit)],
                                  family = "gaussian",
                                  alpha = .5, #0 is ridge
                                  lambda = lambda_elastic)
  
  return_vector <- c()
  
  p <- length(beta_list - 1)
  y <- y %>% as.vector() #so we can do sums of that and our predictions
  for (mod in list(current_lasso, current_ridge,current_elastic)){
    current_preds <- predict(mod, mat_to_fit[,2:ncol(mat_to_fit)] ) %>% as.vector()
    current_bias <- mean(current_preds - y)
    current_var <- var(current_preds)
    current_mse <- sum((current_preds - y)^2) / (num_obs - p - 1)
    return_vector <- c(return_vector, as.vector(coef(mod)), current_bias, current_var, current_mse)
  }
  
  ## for lm
  current_preds <- predict(current_lm, mat_to_fit[,2:ncol(mat_to_fit)] %>% data.frame() ) %>% as.vector()
  current_bias <- mean(current_preds - y)
  current_var <- var(current_preds)
  current_mse <- sum((current_preds - y)^2) / (num_obs - p - 1)
  return_vector <- c(return_vector, as.vector(coef(current_lm)), current_bias, current_var, current_mse)
  
  highest_beta <- (length(beta_list) - 1)
  
  names(return_vector) <- c(paste("LASSO", 0:highest_beta, sep = '_'),
                            paste("LASSO", c("bias", "variance", "mse"), sep = '_'),
                            paste("Ridge", 0:highest_beta, sep = '_'),
                            paste("Ridge", c("bias", "variance", "mse"), sep = '_'),
                            paste("Elastic", 0:highest_beta, sep = '_'),
                            paste("Elastic", c("bias", "variance", "mse"), sep = '_'),
                            paste("Unpenalized", 0:highest_beta, sep = '_'),
                            paste("Unpenalized", c("bias", "variance", "mse"), sep = '_')
                            )
  
  return(return_vector)
}



format_df <- function(input_df){
  ## takes a dataframe returned by replicating model_fit and transforms it so
  ## there is one row per estimate 
  data <- input_df %>% 
    mutate(sim_number = 1:nrow(input_df)) %>% #we have to do this to get pivots to recognize unique rows
    tidyr::pivot_longer(cols = colnames(input_df), names_to = "Estimate", values_to = "Value") %>% 
    separate(col = "Estimate", into = c("Model", "Beta")) %>%
    unite(col = Model_iter, Model, sim_number, sep = "_") %>%
    pivot_wider(names_from = "Beta", values_from = "Value", names_prefix = "Beta_") %>%
    separate(col = Model_iter, into = c("Model", "Simulation")) %>%
    data.frame() %>% 
    rename(bias = Beta_bias, variance = Beta_variance , mse = Beta_mse )
  return(data)
}


beta_boxplots <- function(input_df, format_data = F, beta_num, beta_list, legend = T){ 
  #' Creates a boxplot using each 
  #' input_df: the dataframe (either returned by replicating model_fit or format_df)
  #' format_data: a logical indicating whether input_df needs to be formatted
  #' beta_num: the beta coefficient you want boxplots of
  #' beta_list: the list of beta coefficients
  #' legend: a logical indicating whether we need to include a legend in this plot
  
  if(!format_data){
    data <- input_df
  }
  else{
    data <- format_df(input_df)
  }
  
  my_list <- data[,paste('Beta', beta_num, sep = "_")]
  truth <- beta_list[beta_num + 1] #we want beta_0 to be the intercept
  
  p <- ggplot(data = data, mapping = aes(col = Model, y = my_list)) + 
    geom_boxplot() +
    geom_hline(aes(yintercept = truth), col = "blue") +
    labs(y = "", title = paste('Beta', beta_num, sep = "_")) + 
    scale_x_continuous(breaks = c())
  
  if(!legend){
    p <- p + theme(legend.position = "none")
  }
    return(p)
    
}



plot_all_beta <- function(input_df, beta_list, ncol = 4){
  #' a function taking a dataframe (returned by replicating model_fit) which returns boxplots
  #' input_df: a dataframe created by replicating model_fit
  #' beta_list: the list of true beta coefficients
  #' ncol: the desired number of columns in the resulting plot.
  df <- format_df(input_df)
  
  
  beta_plots <- list()
  for (i in 0:(length(beta_list) - 1)){
    beta_plots[[i + 1]] <- beta_boxplots(input_df = df, format_data = F, beta_num = i, beta_list = beta_list, legend = F)
    beta_plots[[i + 1]]
  }
  beta_plots[[length(beta_list) + 1]] <- beta_boxplots(input_df = df, format_data = F, beta_num = i, beta_list = beta_list, legend = T) %>%  
    ggpubr::get_legend() %>% 
    as_ggplot()
    
  egg::ggarrange(plots = beta_plots, ncol = ncol)
  
}
```

Here are a series of boxplots which show the distribution of point estimates for each of the 11 $\beta$ parameters. Recall that for the simulation study we set $\beta_0 = 2$, $\beta_1 = 1$, $\beta_2 = 0.5$, $\beta_3 = -0.5$, and $\beta_4 = \beta_5 = ... = \beta_9 = \beta_{10} = 0$.

```{r}
# Set Parameters
n_reps <- 500
beta_list <- c(2,  1, .5, -.5, 0, 0, 0, 0, 0, 0, 0)
num_obs <- 300

# Generate Data
future::plan(multisession, workers = parallel::detectCores()) 
df <- future.apply::future_replicate(n_reps, model_fit(beta_list, num_obs)) %>% t() %>% data.frame()
future::plan(sequential)

# Plot Results
plot_all_beta(df, beta_list)
```

We can see from the boxplots of the beta coefficients that LASSO, Ridge, and elastic net tend to underestimate the true parameter, while the unpenalized model is unbiased. However, when parameters are zero, the spread of elastic net and LASSO estimates is much smaller. 

Separately, we are interested in the MSE and variance of each $\beta$. below I plot both of these:

```{r, echo=FALSE}
fdf <- format_df(df)

means <- fdf %>%
  select(-c("bias", "variance", "mse", "Simulation")) %>% 
  group_by(Model) %>%
  dplyr::summarise(across(everything(), .fns = mean, na.rm= TRUE)) 

beta_vars <- fdf %>%
  group_by(Model) %>%
  select(-c("bias", "variance", "mse", "Simulation")) %>% 
  dplyr::summarise(across(everything(), 
                   .fns = ~ sum((.x-mean(.x))^2)/length(.x) ) ) # We're not using the estimate of population variance here, I believe


beta_bias <- sapply(1:nrow(means), function(x){
  unlist(means[x,-1]) - beta_list}) %>%   
  t() %>%  
  cbind(means[,1], . )

beta_mse <- sapply(1:nrow(means), function(x){
  beta_bias[x, -1]^2 + beta_vars[x, -1]
  }) %>% 
  t() %>%  
  cbind(means[,1], . ) 

beta_mse %>% tidyr::pivot_longer(cols = colnames(beta_mse)[-1], names_to = "beta_num", 
                     values_to = "value") %>% 
  separate(beta_num, into = c("junk", "beta_num")) %>% 
  mutate(beta_num = as.numeric(beta_num)) %>% 
  mutate(short_model = substr(Model, start = 1, stop = 2)) %>% 
    arrange(beta_num) %>% 
  mutate(is_zero = (beta_list[beta_num + 1] == 0)) %>% 
  ggplot() + geom_col(mapping = aes(x = short_model, y = value, fill = is_zero)) +
  facet_wrap(vars(beta_num), scales = "free") +
  labs(title = "Mean Squared Error of Regression Coefficient", x = '', y = '', fill = "Is Zero")

```


When evaluating the mean squared error of our estimates of regression coefficients, Elastic Net and LASSO regression tend to have lower MSE for coefficients when they are equal to zero. 

```{r, echo=FALSE}
beta_vars %>% tidyr::pivot_longer(cols = colnames(beta_mse)[-1], names_to = "beta_num", 
                     values_to = "value") %>% 
  separate(beta_num, into = c("junk", "beta_num")) %>% 
  mutate(beta_num = as.numeric(beta_num)) %>% 
  mutate(short_model = substr(Model, start = 1, stop = 2)) %>% 
  arrange(beta_num) %>% 
  mutate(is_zero = (beta_list[beta_num + 1] == 0)) %>% 
  ggplot() + geom_col(mapping = aes(x = short_model, y = value, fill = is_zero)) +
  facet_wrap(vars(beta_num), scales = "free") + 
  labs(x = '', y = '', title = "Variance of Estimators", fill = "Is Zero")
```

The variance in estimates is also much lower for Elastic Net and LASSO regression for zero coefficients, but is comparable to unpenalized regression for non-zero coefficients. 

Now, let's see how these models perform under different conditions. We will evaluate the prediction MSE for each model under different levels of sparsity (.1, .5, and .98), and at different sample sizes (20, 100, 500). We will use 100 replications under each condition to conduct estimation, and we will use 20 covariates in each simulation. 

```{r, echo = FALSE}
df <- read.csv('aggregate_mse_data.csv')


df2 <- df %>% pivot_longer(cols = c("LASSO", "Ridge", "Elastic", "Unpenalized"), 
                           values_to = "MSE", 
                           names_to = "Model") %>% 
  dplyr::rename(Small = prop_small )
```

```{r, echo=FALSE}
ggplot(df2, mapping = aes(col = Model, x = as.factor(num_obs), y = log(MSE))) + 
  geom_boxplot() +
  facet_grid(rows = vars(sparsity), labeller = label_both) +
  labs(x = "Number of Observations")
```

It is clear that in the very high sparsity setting (i.e., the setting where nearly all coefficients are truly 0) we see that the OLS model performs quite poorly compared to the penalization methods (especially when there are fewer observations). A somewhat counter-intuitive finding is that the penalization methods perform quite well on median in the low sparsity setting as well; however there is wide variance in out-of-sample MSE across each simulation in the setting with only 200 observations. A further question of interest might be to examine model performance for even lower sparsity (or even 0 sparsity), and increase the number of covariates tested from 20 to say, 200; and to compare results.   

Now we consider the setting where the non-zero coefficients are close to 0, (i.e., "small"). In this setting, we fix sparsity at [X] and vary the size of non-zero coefficients. We set small coefficients to lie in the range 0.0005-0.005, an we allow the remaining coefficients to lie in the range 0.5-1.5. So for example, in the setting with `small = 0.5`; half of the true non-zero coefficients lie in 0.0005-0.005, and half lie in 0.5-1.5. In this setting, we expect that severe penalization methods like LASSO and Elastic Net will send too many of these coefficients to 0, and the OLS model will overestimate these small true $\beta$s= values. 

```{r, echo=FALSE}
ggplot(df2, mapping = aes(col = Model, x = as.factor(num_obs), y = log(MSE))) + 
  geom_boxplot() +
  facet_grid(rows = vars(Small), labeller = label_both) +
  labs(x = "Number of Observations")
```

Indeed, we see that Ridge regression is the highest performing model in this setting, and this is especially true with a lower number of observations. 


Below we present results for all 9 combinations of sparsity and coefficient size. 

```{r, echo=FALSE}
ggplot(df2, mapping = aes(col = Model, x = as.factor(num_obs), y = log(MSE))) + 
  geom_boxplot() +
  facet_grid(rows = vars(Small), cols = vars(sparsity), labeller = label_both) +
  labs(x = "Number of Observations")
```


As stated in the simulation design portion above, in order to fit our penalization models, we simply used cross validation to select the lambda value associated with the smallest MSE, and then used this lambda value in our final regression fit. A related consideration is to examine results for different levels of `lambda` in LASSO regression, and observe sensitivity of results for this varying levels of lambda.  [Explain what's happening]

```{r, echo = FALSE, message = FALSE}
rm(full_df)

beta_list <- c(2, -1.2, -.7, -.1, .2, .5, 1,0, 0, 0, 0, 0, 0, 0)

for (i in 1:10){

  mat_to_fit <- generate_data(beta_list = beta_list, num_obs = 1000, num_underlying_factors = 1, covariates_per_factor = 4, strength_collinearity = .9)
  
  
  current_lasso <- glmnet::glmnet(y = mat_to_fit[,1], 
                                  x = mat_to_fit[,2:ncol(mat_to_fit)],
                                  family = "gaussian",
                                  alpha = 1 #1 is lasso penalty (0 is ridge)
                                  )
  
  current_df <- current_lasso$beta %>% 
    t() %>% 
    as.matrix() %>% 
    data.frame() %>% 
    cbind(data.frame(lambda = current_lasso$lambda)) %>% 
    pivot_longer(., cols = colnames(.)[colnames(.) != "lambda"])
  
  current_df$iter = paste(i, current_df$name)
  
  if (exists("full_df")){
    full_df <- rbind(full_df, current_df)
  }
  else{
    full_df <- current_df
  }
}


  ggplot(data = full_df) +  geom_line(mapping = aes(x = lambda, y = value,group = iter, col = name 
                                                     ), alpha = .5)

```

[As you can see, ...]

### 4) Summary/Conclusion

### 5) References

- https://www.xlstat.com/en/solutions/features/ordinary-least-squares-regression-ols
- Dr. Lake Slides
- Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. J. R. Statist. Soc. B, 58, 267–288.
- Zou, H., Hastie, T. (2005). Regularization and variable selection via the elastic net. J. R. Statist. Soc. B, 67(2), 301–320.


