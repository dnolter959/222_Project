---
title: "BST 222 Final Reoprt"
author: "Zach Clement, Nona Jiang, Dan Nolte, Willow Duffell, Addison McGhee"
date: "11/30/2021"
output:
  html_document: default
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(pander)
library(tidyverse)
library(ggplot2)
library(ggpubr)
library(future.apply)
library(knitr)
library(dplyr)
library(glmnet)
```

### I) Introduction
The purpose of this study is to examine the impacts of a phenomenon in regression analysis and prediction known as “sparsity”. In plain English, the term sparsity refers to the state of being thinly scattered or distributed. This definition has clear connections to sparsity in a regression model, which we will consider as the percent of true coefficients that are exactly equal to zero. Having high sparsity implies that the model under consideration has many predictors that are not impactful and could therefore be discarded to achieve a more parsimonious, or simple relationship with the outcome. In real world applications, it is often advantageous to obtain a model that has fewer predictors as this allows for ease of interpretation and stronger effects from each individual covariate. For example, in the case of genomics research, a setting where the number of predictors far exceeds the sample size, the number of meaningful (non-zero) features in the data cannot be discerned using the traditional Ordinary Least Squares approach. Even the elegant regularization procedure of Ridge regression is unable to remove parameters since it only reduces the magnitude of coefficients, and thus does not force them to zero.

The above example makes it clear that a more powerful technique is warranted to better estimate our parameters of interest while still maintaining a degree of parsimony in model structure. In 1996, Stanford Statistics Professor Robert Tibishirani published a paper titled “Regression Shrinkage and Selection via the LASSO, which presented the Least Absolute Shrinkage and Selection Operator (LASSO) as an alternative to the existing regularization options. The LASSO introduces a new form of regularization that effectively “shrinks” estimates for nil coefficients to be exactly zero. Fifteen years later, Tibishirani’s Stanford colleagues, Professors Trevor Hastie and Hui Zou, developed the Elastic-Net selection procedure as a way to strike a balance between the flexibility of Ridge regression and the severity of LASSO. There now exist a myriad of estimation approaches, with each having advantages and disadvantages that we will explore further in this paper.

With the motivation given, we will now turn attention to our underlying research questions:

1.) How does sparsity impact the Mean Squared Error of established regression methods like OLS and Ridge, and how do these perform relative to the LASSO and Elastic-Net procedures?

2.) What situations allow for the penalized regression methods to achieve lower variance relative to OLS at the cost of higher bias?

3.) How do the four methods above perform for larger sample sizes and number of simulations?

### 2) Methods

#### a) Models
We will now present the main statistical methods that were utilized in our investigation. In order to examine the effects of sparsity in linear models, we looked at four distinct model methods that deal with sparsity in different ways. These four methods are: 

- Ordinary Least Squares
- The Least Absolute Shrinkage and Selection Operator (LASSO)
- Ridge Regression
- Elastic-Net

##### No Pentalization

###### Ordinary Least Squares
Ordinary Least Squares, or OLS, is a method in linear regression that chooses parameters of a linear model based on a set of explanatory variables based on the principle of least squares. The Least Squares Estimation process is based on the process of minimizing the distance between the actual data responses and the fitted values/residuals of the estimated linear regression line. The line produced by the method of least squares is shown below:
Ordinary Least Squares, or OLS, is a method in linear regression that estimates parameters of a linear model based on a set of explanatory variables based on the principle of least squares. A model with p explanatory variables would be modeled as such:

$Y = \beta_{0} + \sum_{j=1}^{p} \beta_{j}X_{j} + \epsilon$

Where Y is the dependent variable, $\beta_0$ is the intercept, $X_{j}$ is the jth explanatory variable and $\epsilon$ is the random error term. 

Least squares, also known as the minimum square error (SSE), goes about an estimation process that is based on minimizing the sum of square distances between the true data responses and the predicted values. That is, we minimize the *residuals*, which can be defined as $Y_{i} - \hat{Y}_i = e_{i}$. In order to accomplish this goal, the method of least squares produces a line such that 

$Y_{i} - \hat{Y}_{i} = Y_{i} - (\hat{\beta}_{0} + \hat{B}_{1}X_{i}) = e_{i}$

is minimized over all observations by summing up all the squares of the residuals such that the sum $e^{2}_{1} + e^{2}_{2} + \dots + e^{2}_{n}$ is minimized. 


In other words, the ordinary least squares method chooses estimates so that the regression model deviates the least from the data, with no penalty term. 

**Things to note:**
The outputs of regression for the OLS estimator are unbiased estimators of the different $\beta$ terms. According to the Guass-Markov Theorem, each of the $\hat{\beta}$ variables are unbiased and have minimum variance among all unbiased linear estimators for $\beta$. However this estimator will work best with data with no sparsity and large $\beta$ terms. When we look towards models with more sparsity, OLS may not be the best method to use. 



##### Pentalization

Next, we look at other methods that include penalty terms in order to create the most optimal model. These penalization methods have large benefits that allow for regression models to have the best predictive power by either minimizing the number of covariates or minimizing the weight of the covariates. When there are a large number of covariates in a data set, there may be too many to include in a regression model or not all of the covariates may be the best predictors of the regression. The regression methods below offer different penalization/shrinkage for variable selection below. However, there are some disadvantages to these methods. One being the added difficulty of model interpretation and the largest disadvantage being the presence of bias in exchange for the better predictive model. 

The idea of the penalization methods is that conventional regression is first performed and then the constraint is applied to the model depending on the chosen variable selection model (LASSO, Ridge Regression or Elastic Net).


###### LASSO
LASSO regression, also known as Least Absolute Shrinkage and Selection Operator is a type of linear regression that uses shrinkage in a model as penalization. The main goal in LASSO regression is to select and eliminate variables by decreasing the beta coefficients to zero. If a variable is not strongly associated with the outcome, the beta coefficent is decreased to zero and is therefore taken out of the model. This regression model can also be seen as a model selection method since the model takes out what is deems as "insignificant predictors". 



LASSO regression performs L1 regularization, which adds a penalty equal to the absolute value of the magnitude of coefficients. In other words, L1 regularization adopts the constraint that the sum of the absolute-valued regression coefficients must be less than some threshold. The goal of LASSO regression is to minimize:

$\sum^{n}_{i=1} \big( Y_{i} - \beta_{0} - \sum^{p}_{j=1} \beta_{p}X_{ij} \big)^{2} + \lambda\sum^{p}_{j=1}|\beta_{j}|$


which is the same as minimizing the sum of squares. The tuning parameter $\lambda$, (either positive or 0) controls the strength of the L1 penalty and therefore the amount of shrinkage in the model. So when $\lambda=0$, LASSO will equal the LSE. However, with this tuning parameter comes a balance of bias and variance. As $\lambda$ increases the bias of the model will increase, but when $\lambda$ decreases, the variance of the model will increase. Since the goal is to have the best predictive model with the least amount of bias and variance, that must be taken into account when choosing the tuning parameter if LASSO regression is chosen as the regression model method.



*Things to note:* The LASSO method is the preferred method when working with relatively small number of predictors that have a have a substantial effect, and the remaining predictors have coefficients that are very small or to equal zero. Therefore this method will be much more effective with a model with more zero's or sparsity. LASSO regression has many benefits that other methods don't have. This method results in a much simpler model that is much easier to interpret and it removes collinear/multicollinear variables by keeping only one of the correlated terms and eliminating the others. However, in certain settings LASSO regression may be too harsh of a method and may reduce the model by estimating more coefficients than wanted, resulting in a oversimplified model with lost information.







###### Ridge Regression

Ridge Regression is yet another regression model with a penalization element, but it more similar to Least Squares. Ridge regression estimates coefficients of the model by minimizing the SSE but also also constrains the sums of squared coeffiencts. This method incorporates what is known as the L2 penalty which contains the constraint that the sum of the $p^{2}$ regression coefficients must be less than some threshold. This L2 penalty is used to minimize the SSE, as seen below:


$\sum^{n}_{i=1} \big( Y_{i} - \beta_{0} - \sum^{p}_{j=1} \beta_{p}X_{ij} \big)^{2} + \lambda\sum^{p}_{j=1}\beta_{j}^2$


$\lambda$ here again is the tuning parameter/shrinking penalty and will shrink the estimates of $\beta$ towards to zero, but will not actually ever set a $\beta$ coefficient to zero. The largest advantage Ride Regression has over Least Squares is in the bias-variance trade-off. As the tuning parameter, $\lambda$, increases, the greater the shrinkage in the model, the lesser amount of flexibility since the $\beta$ coefficients are going towards zero in a somewhat proportional manner. Therefore, we have decreased bias, but an increase in variance. On the other hand, as $\lambda$ decreases, the $\beta$ coefficients goes towards the OLS method which leads to decreased variance but increased bias in the model.  



*Things to note:* Ridge Regression has a lot of factors that allow it to work better in specific model cases. Ridge regression will work best when the least squared estimates have high variance, potentially when small changes in the data causes a large change in the least squared estimate. Another instance where ridge regression works well is when the number of covariates is about the same as the number of observations in the data set. Furthermore, this model is preferred when all or most of the predictors are important variables when predicting the outcome, since all the variables will be used in the model. While there are several instances where ridge regression would be the preferred method there are some disadvantages with the model. First, while the penalty term in ridge regression will shrink coefficient estimates towards zero, it will not set any of those estimates to zero. Therefore the final model includes all of the initial predictors which could potentially cause issues with high dimensionality, parsimony, interpretation and the inclusion on collinear terms. 





###### Elastic Net

Due to criticisms of the other penalization methods, elastic net regression was created as a happy medium between ridge regression and LASSO regression. This solution incorporates the combination of the penalties of ridge regression and LASSO in order to get the best of both models. The penalty term of elastic net includes 2 $\lambda$ values, one penalty term for ridge regression and one penalty term for LASSO regression. Cross validation is used on different combinations for these two tuning parameters in order the find their optimal values. This way, the resulting equation is a nice balance of the L1 and L2 penalty impacts. Elastic net regression accomplishes this goal by minimizing the following loss function:


$\frac{1}{2n}\sum^{n}_{i=1} \big( Y_{i} - X_{i}\hat{\beta} \big)^{2} +  \lambda \big( \frac{1-\alpha}{2}\sum^{p}_{j=1}\beta_{j}^2 + \alpha \sum^{p}_{j=1}|\hat{\beta}_{j}|\big)$


where $\alpha$ is an additional tuning parameter such that
* if $\alpha = 0$, the regression approach becomes ridge regression
* if $\alpha = 1$, the regression approach becomes LASSO regression
* if $\alpha \in (0,1)$, the regression approach is Elastic Net regression


*Things to note:* Elastic net incorporates the best of both methods. This approach is not necessarily the best choice all the time, it depends on the given data and covariates. This approach will work best with variables with a medium amount of sparsity


#### b) Metrics

[I wrote this below in the results section but it might be better suited for here. - Dan.]

Out of Sample MSE:
To calculate out-of-sample MSE, we take each model fit and compare true $Y$ value to predicted $\hat{Y}$ values, and compute $\frac{\sum_{i = 1}^{n}{(Y_i - \hat{Y_i})^2}}{n}$, in the typical way. However, crucially, we do not calculate this MSE on the same data that was used to fit the model. Instead, we generate a new random data sample, call it test data, and calculate associated true ${Y}$ values using this new sample and the true beta coefficients. Then we use the test data and *modeled coefficients* from the original sample to generate predicted ${\hat{Y}}$ values. Then we evaluate prediction error on the test data, instead of the data that was itself used to generate the $\beta$ estimates. We do this because we cannot accurately assess prediction error by testing our model on our training set. For example, the OLS model is constructed to select the $\beta$ estimates which will yield the minimum squared error. This means our MSE will be as small as possible in the OLS model - but this says nothing about how this model can be used for inference with a new dataset. Out-of-sample MSE avoids this problems and allows us to make a fair model comparison. 


#### c) Design of Simulation Study

As discussed above, through this study we were interested in demonstrating the superiority of LASSO regression in those contexts with a high level of sparsity. We then consider other contexts where Ridge Regression, Elastic Net, and OLS are expected to outperform LASSO, and test our hypotheses using simulations. 

##### Step 1: Simulation set up and data generation
We begin with a basic example: we contrive a scenario where we would like to predict Y given a set of 10 covariates using the linear model below. 

${Y} = {\beta}_0 + {\beta}_1 x_1 + {\beta}_2 x_2 + ... + ... +{\beta}_{10} x_{10}$

We then set the true parameters as follows:

${Y} = 2 + 1 x_1 + 0.5 x_2 -0.5 x_3 + 0 x_4 + 0 x_5 + 0 x_6 + 0 x_7 + 0 x_8 + 0 x_9 + 0 x_{10}$

Note that 7 of the parameters are set to 0; we made this choice because we are primarily interested in comparing model performance in the presence of sparsity. 

Next, we generate a random dataset, and then run our 4 models of interest (Unpenalized OLS, LASSO, Ridge, Elastic Net) on this dataset to estimate each of the 11 $\beta_i$ values using each model. We then repeat this simulation and fit regression coefficients for each model 500 times, and compare the performance of the four models. Below we explain this process in greater detail:  

**Step 1a: Generating random data**

In order to generate our random dataset we choose a sample size of 300, and sample from a standard normal distribution for every coviariate value for each of the 300 observations. Again, every single value of every covariate is sampled from the exact same standard normal distribution. We do this to introduce randomness into the data generating process while still adhering to the normality and equal variance assumptions required by the estimation procedures we wish to test. The head of this data frame looks like this:

```{r, echo=FALSE}
beta_list <- c(2,  1, .5, -.5, 0, 0, 0, 0, 0, 0, 0)
num_obs = 6 
xi_sd <- replicate(n = length(beta_list) - 1, rnorm(num_obs))
data.frame(xi_sd)
```

Using these randomly generated covariate values, we generate corresponding Y values given our choice of parameters by simply multiplying the design matrix corresponding to our random data by the vector of parameter values, **and then adding an error term** (as drawn from the standard normal distribution). We must add an error term as per the specification of the linear model, and because otherwise OLS would exactly predict our parameter values in each simulation. Doing so we retrieve a single dataset of randomly drawn covariate values and their corresponding Y values given our choice of parameters. The head of this data frame looks like this:

```{r, echo=FALSE}
x = cbind(rep(1, num_obs), xi_sd)
y = beta_list %*% as.matrix(t(x)) + rnorm(num_obs) # create matrix with TRUE Y
data <- cbind(y = t(y), xi_sd)
data = data.frame(data)
names(data) <- c("Y", "X1", "X2", "X3", "X4", "X5", "X6", "X7", "X8", "X9", "X10")
data
```

**Step 2: Simulations Details**

Once we have our dataset of predictors $X_i$ and outcome $Y$, what remains is to run our four regression models of interest on this dataset to retrieve point estimates for the 11 $\beta$ coefficients. After running each of the models, we examine (1) the proximity of the estimated coefficients to their true underlying values in each case, as well as (2) the proximity of the predicted $\hat{Y}$ values to their true $Y$, measured in Mean Squared Error. 

We then complete this experiment 500 times and examine the variance of the distributions of the coefficient point estimates.

As for the details of our regression models, we'll consider them in two categories: (1) the un-penalized model (OLS), and (2) the penalization models (Ridge, LASSO, Elastic Net)   

1) *Un-penalized Model (OLS)*:
For this model, we simply regress Y on $X_1, X_2, ..., X_{10}$  using OLS using the below R command. 

```{r, results="hide"}
lm(Y ~ X1:X10, data = data)
```

2) *Penalization Models*:
We used the same basic procedure for fitting each of these 3 models. For each we use the `glmnet` library in R and run a model in this form:

```{r, echo = FALSE}
alpha = 0.5
lambda = 0.6
```

```{r, results="hide"}
glmnet(y = data$Y, x = data[,2:ncol(data)], family = "gaussian", alpha = alpha, lambda = lambda)
```

Where we set alpha equal to 0, 0.5, and 1 for Ridge, Elastic Net, and LASSO, respectively. Of course, a more rigorous study would also attempt to select an optimal alpha level for the Elastic Net model, but choosing alpha = 0.05 for Elastic Net is sufficient for our illustrative purposes.

As for selecting the lambda level, used 5-fold cross validation across many levels of lambda and selected the lambda in each case associated with the lowest average MSE in the test set. Then we fit each penalization model with this lambda value. The R code used to retrieve the optimal lambda level is stated below:

```{r, results="hide", message = FALSE, warning=FALSE}
cv.glmnet(y = data$Y, x = as.matrix(data[,2:ncol(data)]),
          nfolds = 5, family = "gaussian", alpha = alpha) %>% .$lambda.min
```

### 3) Results

Below are a series of boxplots which show the distribution of point estimates for each of the 11 $\beta$ parameters. Recall that for the simulation study we set $\beta_0 = 2$, $\beta_1 = 1$, $\beta_2 = 0.5$, $\beta_3 = -0.5$, and $\beta_4 = \beta_5 = ... = \beta_9 = \beta_{10} = 0$. In this setting we set n = 300, and run each model 500 times. 

```{r, echo=FALSE}
# Load Simulation Functions
source("analysis_scripts/generate_data.R")
source("analysis_scripts/model_fit.R")
source("analysis_scripts/plot_functions.R")

# Set Parameters
n_reps <- 500
beta_list <- c(2,  1, .5, -.5, 0, 0, 0, 0, 0, 0, 0)
num_obs <- 300

# Generate Data
future::plan(multisession, workers = parallel::detectCores()) 
df <- future.apply::future_replicate(n_reps, model_fit(beta_list, num_obs)) %>% t() %>% data.frame()
future::plan(sequential)

# Plot Results
plot_all_beta(df, beta_list)
```

We can see from the boxplots of the beta coefficients that LASSO, Ridge, and elastic net tend to underestimate (in absolute value terms) the true parameter, while the unpenalized model is unbiased.  This jibes with our expectation, as the penalization methods are shrinking the truly non-zero coefficients (often all the way to 0 in the case of LASSO), and in so doing are introducing bias in exchange for smaller variance. The clear superiority of Unpenalized OLS on predicting $\beta_1$, $\beta_2$, and $\beta_3$ should serve as a cautionary tale that these penalization methods, while powerful, certainty introduce bias when predicting relatively "large" parameter values and should be used with care.

On the other hand, when parameters are truly zero, -- despite the fact that Unpeanlized OLS have very low bias in these cases -- the penalization methods outperform Unpenalized OLS in that they have much lower variance; in other words, they more accurately predict the true 0 betas much more frequently than do does Unpenalized OLS. 

It's also important to note that in this example Ridge does not perform much better than Unpenalized OLS for the non-zero coefficients in terms of variance. This is partly due to the fact that Ridge does not send any coefficients to 0; but it's slighter smaller variance for these parameter estimates is due to the shrinkage introduced by theL1 penalty term.  

Separately, we are interested in the MSE and variance of each $\beta$. below I plot both of these:

```{r, echo=FALSE}
source("analysis_scripts/calculate_beta_MSE.R")
beta_MSE = calculate_beta_MSE(df)

beta_MSE %>% ggplot() + geom_col(mapping = aes(x = short_model, y = value, fill = is_zero)) +
  facet_wrap(vars(beta_num), scales = "free") +
  labs(title = "Mean Squared Error of Regression Coefficient", x = '', y = '', fill = "Is Zero")
```

```{r, echo=FALSE}
source("analysis_scripts/calculate_beta_variances.R")
beta_variances = calculate_beta_variances(df)

beta_variances %>% ggplot() + geom_col(mapping = aes(x = short_model, y = value, fill = is_zero)) +
  facet_wrap(vars(beta_num), scales = "free") + 
  labs(x = '', y = '', title = "Variance of Estimators", fill = "Is Zero")
```

When evaluating the mean squared error of our estimates of regression coefficients, Elastic Net and LASSO regression tend to have lower MSE for coefficients when they are equal to zero; this finding is consistent with the boxplots above. Again, MSE is significnatly lower for truly non-zero parameters when estimated with Unpenalized OLS. As for variance, the variance in $\beta$ estimates is much lower for Elastic Net and LASSO regression for zero coefficients, but is comparable to Unpenalized OLS for non-zero coefficients.

We've now reviewed the estimates of the *beta coefficients* relative to their true underlying values. A similar but related question we'd like to answer is how well each model performs at predicting the true underlying *Y values*. In order to do this, we compare models using **out-of-sample MSE.** 

To calculate out-of-sample MSE, we take each model fit and compare true $Y$ value to predicted $\hat{Y}$ values, and compute $\frac{\sum_{i = 1}^{n}{(Y_i - \hat{Y_i})^2}}{n}$, in the typical way. However, crucially, we do not calculate this MSE on the same data that was used to fit the model. Instead, we generate a new random data sample, call it test data, and calculate associated true ${Y}$ values using this new sample and the true beta coefficients. Then we use the **test data** and **modeled coefficients** from the original sample to generate predicted ${\hat{Y}}$ values. Then we evaluate prediction error on the test data, instead of the data that was itself used to generate the $\beta$ estimates. We do this because we cannot accurately assess prediction error by testing our model on our training set. For example, the OLS model is constructed to select the $\beta$ estimates which will yield the minimum squared error. This means our MSE will be as small as possible in the OLS model - but this says nothing about how this model can be used for inference with a new dataset. Out-of-sample MSE avoids this problems and allows us to make a fair model comparison.

```{r, echo=FALSE}
mse_1 = read.csv('data/original_set_out_sample_mse.csv')
mse_1 = mse_1 %>% pivot_longer(
    cols = c("LASSO", "Ridge", "Elastic", "Unpenalized"), 
    values_to = "MSE", 
    names_to = "Model")
```

```{r, echo=FALSE}
ggplot(mse_1, mapping = aes(col = Model, x = 0, y = log(MSE))) + 
  geom_boxplot() +
  theme(axis.title.x=element_blank(),
      axis.text.x=element_blank(),
      axis.ticks.x=element_blank())
```

It is clear that in this setting of relatively high sparsity, Elastic Net and LASSO outperform Ridge and un-penalized OLS in terms of out-of-sample MSE. This is because bias introduced by shrinking the non-zero parameters in the penalized models is only a small price to pay for the steep reduction in variance when predicting the truly 0 parameter values. In this setting where there is high sparsity, it is clear that LASSO reigns supreme. 

However, what if there is slightly less sparsity? Or what if the true parameters are non-zero but instead just very small? How does each model perform in these settings. We now proceed to test some of these questions to better understand the trade offs implicit in these penalization methods.

First, we will evaluate the prediction MSE for each model under different levels of sparsity (.1, .5, and .98), and at different sample sizes (20, 100, 500). A sparsity of 0.1 means that 10% of the true beta coefficients are 0. We will use 100 replications under each condition to conduct estimation, and we will use 20 covariates in each simulation. 

```{r, echo=FALSE}
mse_2 <- read.csv('data/aggregate_mse_data.csv')
mse_2 <- mse_2 %>% pivot_longer(
    cols = c("LASSO", "Ridge", "Elastic", "Unpenalized"), 
    values_to = "MSE", 
    names_to = "Model"
  ) %>% 
    dplyr::rename(Small = prop_small)

```

```{r, echo=FALSE}
mse_2_fixed_smallness = mse_2 %>% filter(Small == 0)
ggplot(mse_2_fixed_smallness, mapping = aes(col = Model, x = as.factor(num_obs), y = log(MSE))) + 
  geom_boxplot() +
  facet_grid(rows = vars(sparsity), labeller = label_both) +
  labs(x = "Number of Observations")
```

It is clear that in the very high sparsity setting (i.e., the setting where nearly all coefficients are truly 0) we see that the OLS model performs quite poorly compared to the penalization methods (especially when there are fewer observations). A somewhat counter-intuitive finding is that the penalization methods perform quite well on median in the low sparsity setting as well; this can be explained by the fact that in these settings, cross-validation will set the optimal value of $\lambda$ to close to 0. A further question of interest might be to examine model performance for even lower sparsity (or even 0 sparsity), and increase the number of covariates tested from 100 to say, 200; and to compare results.   

Now we consider the setting where the non-zero coefficients are close to 0, (i.e., "small"). In this setting, we fix sparsity at 0.1 (i.e., 10 of the 100 parameters are truly 0) and vary the size of non-zero coefficients. We set small coefficients to lie in the range 0.01-0.1, an we allow the remaining coefficients to lie in the range 0.5-1.5. So for example, in the setting with `small = 0.5`; half of the true non-zero coefficients lie in 0.01-0.1, and half lie in 0.5-1.5. In this setting, we expect that severe penalization methods like LASSO and Elastic Net will send too many of these coefficients to 0, and the OLS model will overestimate these small true $\beta$ values. 

```{r, echo=FALSE}
mse_2_fixed_sparsity = mse_2 %>% filter(sparsity == 0.1)
ggplot(mse_2_fixed_sparsity, mapping = aes(col = Model, x = as.factor(num_obs), y = log(MSE))) + 
  geom_boxplot() +
  facet_grid(rows = vars(Small), labeller = label_both) +
  labs(x = "Number of Observations")
```

Indeed, we see that Ridge regression is the highest performing model in the setting of high "smallness", and this is especially true with a lower number of observations. However, as sparsity increases, LASSO and Elastic Net begin to perform as well or better than Ridge Regression.

Below we present results for all 9 combinations of sparsity and coefficient size. 

```{r, echo=FALSE}
ggplot(mse_2, mapping = aes(col = Model, x = as.factor(num_obs), y = log(MSE))) + 
  geom_boxplot() +
  facet_grid(rows = vars(Small), cols = vars(sparsity), labeller = label_both) +
  labs(x = "Number of Observations")
```

### 4) Summary/Conclusion

### 5) References

- https://www.xlstat.com/en/solutions/features/ordinary-least-squares-regression-ols
- Dr. Lake Slides
- https://www.statisticshowto.com/lasso-regression/
- Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. J. R. Statist. Soc. B, 58, 267–288.
- Zou, H., Hastie, T. (2005). Regularization and variable selection via the elastic net. J. R. Statist. Soc. B, 67(2), 301–320.


