---
title: "BST 222 Final Reoprt"
author: "Dan Nolte"
date: "11/30/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Introduction
The purpose of this study is to examine the impacts of a phenomenon in regression analysis and prediction known as “sparsity”. In plain English, the term sparsity refers to the state of being thinly scattered or distributed. This definition has clear connections to sparsity in a regression model, which we will consider as the percent of true coefficients that are exactly equal to zero. Having high sparsity implies that the model under consideration has many predictors that are not impactful and could therefore be discarded to achieve a more parsimonious, or simple relationship with the outcome. In real world applications, it is often advantageous to obtain a model that has fewer predictors as this allows for ease of interpretation and stronger effects from each individual covariate. For example, in the case of genomics research, a setting where the number of predictors far exceeds the sample size, the number of meaningful (non-zero) features in the data cannot be discerned using the traditional Ordinary Least Squares approach. Even the elegant regularization procedure of Ridge regression is unable to remove parameters since it only reduces the magnitude of coefficients, and thus does not force them to zero.

The above example makes it clear that a more powerful technique is warranted to better estimate our parameters of interest while still maintaining a degree of parsimony in model structure. In 1996, Stanford Statistics Professor Robert Tibishirani published a paper titled “Regression Shrinkage and Selection via the LASSO, which presented the Least Absolute Shrinkage and Selection Operator (LASS) as an alternative to the existing regularization options. The LASSO introduces a new form of regularization that effectively “shrinks” estimates for nil coefficients to be exactly zero. Fifteen year later, Tibishirani’s Stanford colleagues, Professors Trevor Hastie and Hui Zou, developed the Elastic-Net selection procedure as a way to strike a balance between the flexibility of Ridge regression and the severity of LASSO. There now exist a myriad of estimation approaches, with each having advantages and disadvantages that we will explore further in this paper.

With the motivation given, we will now turn attention to our underlying research questions:

1.) How does sparsity impact the Mean Squared Error of established regression methods like OLS and Ridge, and how do these perform relative to the LASSO and Elastic-Net procedures?

2.) What situations allow for the penalized regression methods to achieve lower variance relative to OLS at the cost of higher bias?

3.) How do the four methods above perform for larger sample sizes and number of simulations?

### Methods

We will now present the main statistical methods that were utilized in our investigation. In order to examine the effects of sparsity in linear models, we looked at four distinct model methods that deal with sparsity in different ways. These four methods are: 

- Ordinary Least Squares
- The Least Absolute Shrinkage and Selection Operator (LASSO)
- Ridge Regression
- Elastic-Net

#### Ordinary Least Squares
Ordinary Least Squares, or OLS, is a method in linear regression that estimates parameters of a linear model based on a set of explanatory variables based on the principle of least squares. A model with p explanatory variables would be modeled as such:

$Y = \beta_{0} + \sum_{j=1}^{p} \beta_{j}X_{j} + \epsilon$

Where Y is the dependent variable, $\beta_0$ is the intercept, $X_{j}$ is the jth explanatory variable and $\epsilon$ is the random error term. 

Least squares, also known as the minimum square error (SSE), goes about an estimation process that is based on minimizing the sum of square distances between the true data responses and the predicted values. That is, we minimize the *residuals*, which can be defined as $Y_{i} - \hat{Y}_i = e_{i}$. In order to accomplish this goal, the method of least squares produces a line such that 

$Y_{i} - \hat{Y}_{i} = Y_{i} - (\hat{\beta}_{0} + \hat{B}_{1}X_{i}) = e_{i}$

is minimized over all observations by summing up all the squares of the residuals such that the sum $e^{2}_{1} + e^{2}_{2} + \dots + e^{2}_{n}$ is minimized. 


In other words, the ordinary least squares method chooses estimates so that the regression model deviates the least from the data, with no penalty term. 

**Things to note:**
The outputs of regression for the OLS estimator are unbiased estimators of the different $\beta$ terms. According to the Guass-Markov Theorem, each of the $\hat{\beta}$ variables are unbiased and have minimum variance among all unbiased linear estimators for $\beta$. However this estimator will work best with data with no sparsity and large $\beta$ terms. When we look towards models with more sparsity, OLS may not be the best method to use. 



#### LASSO
Works better with more zero's

#### Ridge Regression
Works better with a lot of small beta's (0.01 - 0.05)

#### Elastic Net
Works better with medium sparsity


### Metrics

### Design of Simulation Study  


### Results


### Summary/Conclusion



### Refrences

https://www.xlstat.com/en/solutions/features/ordinary-least-squares-regression-ols
Dr. Lake Slides

